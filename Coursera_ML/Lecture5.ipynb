{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    Theta2_ = Theta2.copy()\n",
    "    Theta2_[:,0] = 0\n",
    "    Theta1_ = Theta1.copy()\n",
    "    Theta1_[:,0] = 0\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    #J######################################################\n",
    "    ones_ = np.ones((X.shape[0],1))\n",
    "    a1 = np.append(ones_,X,axis=1).transpose()\n",
    "    \n",
    "    z2 = Theta1@a1\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    ones_ = np.ones(a2.shape[1])[np.newaxis]\n",
    "    a2 = np.append(ones_,a2,axis=0)\n",
    "    \n",
    "    z3 = Theta2@a2\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    #With this we already have h_theta(x)\n",
    "    #Now let's focus on transforming y_i\n",
    "    \n",
    "    y_ = []\n",
    "    for i in y:\n",
    "        aux = np.zeros(num_labels)\n",
    "        aux[i] = 1\n",
    "        y_.append(aux.copy())\n",
    "        \n",
    "    y_ = np.array(y_)\n",
    "    a3 = a3.T\n",
    "    \n",
    "    #Finally, let's compute the value:\n",
    "    J =  (-1/m) * (y_*np.log(a3) + (1-y_)*np.log(1-a3)).sum() \n",
    "    J += (lambda_/(2*m))*((Theta1[:,1:]**2).sum()+(Theta2[:,1:]**2).sum())\n",
    "    \n",
    "    ##################################################################\n",
    "    #Now let's move on to the next part:\n",
    "    delta3 = np.zeros(num_labels)\n",
    "    delta2 = np.zeros(hidden_layer_size)\n",
    "    \n",
    "    delta3 = (a3 - y_).T\n",
    "    delta2 = (Theta2.T@delta3) * a2 * (1 - a2)\n",
    "    \n",
    "    Theta2_grad = delta3 @ a2.T / m\n",
    "    #We change in Theta1 and not in Theta2 because in Theta2 we did not have\n",
    "    #an extra node for the bias\n",
    "    Theta1_grad = (delta2 @ a1.T)[1:,:] / m\n",
    "    \n",
    "    #if(Theta2_grad.shape != Theta2.shape or Theta1_grad.shape!= Theta1.shape):\n",
    "    #    print(Theta1_grad.shape, Theta2_grad.shape)\n",
    "    #    print(Theta1.shape,Theta2.shape)\n",
    "    \n",
    "    Theta2_grad += lambda_*(Theta2_)/m\n",
    "    Theta1_grad += lambda_*(Theta1_)/m\n",
    "    \n",
    "    # ================================================================\n",
    "    # Unroll gradients\n",
    "    # grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random Initialization\n",
    "\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{l}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Initialize W randomly so that we break the symmetry while training\n",
    "    the neural network. Note that the first column of W corresponds \n",
    "    to the parameters for the bias unit.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    W = epsilon_init - 2 * epsilon_init * np.random.random((L_out, 1 + L_in))\n",
    "\n",
    "    # ============================================================\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Main takeaways about the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta1 = np.zeros((3,3))\n",
    "Theta2 = np.ones((3,3))\n",
    "np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
